:bibtex-file: library.bib
:bibtex-order: alphabetical
:bibtex-style: ieee
:stem: asciimath

= Engineering Generalizable Features for Eye-Tracking Data Through a Cloud-Based Machine Learning Platform
:toc:

== Introduction


---
Michalis Comments on the Introduction:

The introduction is used to provide the MOTIVATION of your thesis to the reader. It explains why you did the work you did. This is the primary purpose of the Introduction. The current version does that only in the "So why would this be useful?" part. Here is some advice on what you can address and how you can structure your introduction.

- The topic. A description of issues that are relevant to your topic. This part of the introduction begins with an umbrella of information about the topic and then narrows to the specific focus of the new study (normally the last sentence of this section/paragraph tells how this connects to the thesis (this is currently missing from the introduction, the connection between the various subsections you have). Definitions of key terms are provided. In this section you carefully select previously published articles to establish a foundation for the new study that is being reported. Drawing on popular press can be effective here if recent news items or data from articles support your cause.

- The Gap. Gap between what is known and what needs to be known. Introducing the challenges or, similarly, a problem can be framed as an opportunity. Whether you are solving a problem or seizing an opportunity, motivate your work by connecting it to things that matter. Near the end of this section, the researcher establishes a gap in the previously reported studies and what questions still need to be answered (e.g., Here are the major papers dealing with this very important topic and here is what they have accomplished. Here are the major flaws/omissions/neglected issues of these papers). This is an important part of the published study because it tells the reader how the new study’s findings relate and add to the knowledge base. For instance, you can use some of the introduction from the IMWUT paper, but focus on eye-tracking (as we focus on physio and facial data there).
- The Goal. After describing the Gap, you need to describe what is the goal of this study. You can/should also describe the research question(s) or inquiry of the study. Sometimes the inquiry question(s) or statement will be in its own section; other times it will be added in the introduction. Either ways this part of the introduction needs to provide a certain description of the goal and the respective research objectives/questions. This section needs to clarify how the proposed research objectives/questions address the previously identified flaws/omissions/neglected issues and why it is important to address them (not every flaw/omission/neglected issue is necessarily important; maybe it was not addressed because it is trivial…).
- How this contributes. A good way to end your Introduction is by framing the contributions that your work makes. In the IMWUT paper you can see the contribution structured as a bulleted or numbered list within a paragraph. Try to be as clear as possible.

Also describing the structure of the thesis in the introduction is something many students are doing.
It will be nice to also have a look in some previous theses, for inspitation on how to structure this section.
For instance, see here Camila's https://ntnuopen.ntnu.no/ntnu-xmlui/handle/11250/2624223

---


Eye-tracking has emerged as a promising non-invasive way to evaluate the effect a task has on a person.


We are developing a set of generalizable eye-tracking features that can predict cognitive performance and a machine learning platform to facilitate these features' testing and development.
Core to our methodology is the Feature Generalizability Index developed by Sharma et al. citenp:[sharmaAssessingCognitivePerformance2020]

=== Cognitive Workload

When engineering generalizable features, we are attempting to identify a relationship between the data and the factor we are trying to predict.
As such, it makes no sense to have generalizable features that can predict arbitrary labels.
We have decided to focus our efforts on predicting cognitive workload.

Cognitive workload describes the level of mental resources that one person requires for performing a task.
The perceived workload can be affected by many different factors such as the complexity of the task, the task's size, and external factors like how well one slept that night.
Cognitive workload is a valuable way of talking about how "heavy" a task is to complete.

Cognitive workload will be an essential concept in any system that helps users learn or make decisions.
For this reason, we have decided to focus our efforts on predicting cognitive workload.
However, cognitive workload is not a concrete metric; it is not something we can measure directly.

As an alternative, we have decided to work with cognitive performance.
By cognitive performance, we mean some measure that correlates to the quality of cognitive work.
Our project will include data gathered from studies handling many different cognitive tasks.
Therefore, we will argue that each task has some measure, such as a score, that correlates to the cognitive performance for that task.

=== Feature Generalizability
Feature generalizability is the degree to which a given feature, extracted from one context, is applicable in predictions on data gathered from other contexts.

When we are successful in predicting cognitive performance within one context, two things could be happening.
The first possibility is that we have identified some patterns or features in the dataset that correlate to cognitive performance within the experiment's context.
For example, suppose an exam score is our measure of cognitive performance. In that case, we could assume that hours spent studying for that exam would be a good predictor of one's performance, with a relatively high degree of context specificity.
The other possibility is that we have found some pattern or feature directly related to cognitive performance without being linked closely to the context.
Studying for a specific test would probably give one good results on that test. However, being well-rested would be closely linked to one's performance while not closely linked to that particular test.

We hypothesize that when developing these generalizable features, some pattern in the eye-tracking data correlates directly to cognitive performance and not merely correlates given the specific context.
Our goal in this thesis will be to identify and engineer a set of features that exhibit this underlying relationship between themselves and cognitive performance.

So why would this be useful?
As a rule of thumb, machine learning needs sufficiently large datasets to provide good results.
However, there are certain domains where predictive power would be helpful, but the necessary data is unavailable or hard to obtain.
(Maybe add some examples of these domains with references)
Feature generalizability could be a technique to utilize data gathered in separate but related contexts to achieve good results in even data-poor environments.

Transfer learning, another popular approach to data-poor contexts, is related to feature generalizability; however, they are distinct.
In transfer learning, through different techniques, one would train a model partially on a domain or context where there is a large amount of data available and then adapt that model to the context with less available data.

Another related but distinct technique from feature generalizability is the expert knowledge an experienced data scientist accumulates throughout several projects.
An experienced data scientist or a subject matter expert could have a priori knowledge about which features typically perform well for a given context or domain.

Feature generalizability could be said to exist in the space between these two approaches to the issue.
It is not developing a model adapted to the problem at hand when necessary. Neither is it not understanding which features would typically be good to use for a specific problem.
Feature generalizability is understanding which features could be extracted from one dataset and build models that could predict in another related dataset.


==== Feature Generalizability Index (FGI)

To measure feature generalizability, we will follow the method laid out by Sharma et al. citenp:[sharmaAssessingCognitivePerformance2020].
Their method provides us with a Feature Generalizability Index (FGI) calculated using ANOSIM (Analysis of similarity).
To measure how generalizable our features, we need a statistical test to see the similarities between the tests we run in our in-study and our out-of-study experiments.
We have used NRMSE to measure the error in our predictions.
As there is no theoretical distribution that describes the NRMSE values, we need a non-parametric test to compare our two distributions.
The FGI method uses ANOSIM (Analysis of similarities) to do this.
ANOSIM is a non-parametric test that bears the null hypothesis that two or more groups have a different mean and variance.
Our groups will be the NRMSE-values from the in-study-tests and the NRMSE values from the out-of-study-tests


== Related Work

---
Michalis Comments on the RW:

The primary function of the RW section is to answer the following four questions: “What are the "major/important" relevant works in this topical area? What did they do? What did they find? and How is this work here different? (e.g., might be extending or complementing previous works)”. This last item you need to consider when writing this section is the differentiation of your work (compared to the related studies and works), this is very important since it allows the reader to see the big picture of your study and how your study furthers our knowledge in this particular field.

RW section should not read like a shopping list of who-did what. It should offer insights and education about prior work, and portray the recent developments of the field. It should help readers understand the prior work better than they did before. It is advisable to break this section into sub-sections (themes), this will allow the reader to grasp the various themes better. For instance, if the paper introduces an novel pipeline on extracting generalizable features from gaze data, it is reasonable to report the related work on (1) generalizable features on any human data, (2) gaze data and their particularity, and (3) pipelines that do similar job. The state-of-the art as well as the differentiation of the current work from prior work can then be achieved on a per-theme basis, rather than differentiating against every piece of prior work raised. In the current version, I don't see the nessesery narration (i.e. how this work differs/adds VS previously published work).

The craft of writing this section is based on writers ability to describe what has been done and how this study furthers our knowledge. Thus, it is important to be able to craft the "differentiation" part nicely, without being defensive to previous works and keep a nice narration/flow that nurtures readers' understanding. It is not required that the current work assert itself as “better” than prior work. Rather, current work can be different than prior work and adequately contribute to an important research objective. Asking a different question, using a different method or technique, building on different technological affordances, or focusing on different effects of these affordances (e.g., not necessarily to learning itself, but also on students' motivation and engagement). It is common that the last sentence of each paragraph summarizes the paragraph and provides insights to motivate the next paragraph. At the end of this section, it is important that the reader sees the gaps this study intents to close and the importance of closing this gap.

Again, see examples from previous theses.

---


=== Eye Tracking

Eye-tracking uses devices and software to track and record the position of a subject's eyes while interacting with digital devices. Eye-tracking can be used for input control or recording behavior during interactions with a system.

As the technology has improved and systems become cheaper and cheaper, eye-tracking has emerged as an effective, efficient, and cheap non-invasive method of tracking attention and cognitive workload and many other factors.

There are several different ways of performing eye-tracking. We are working with optical eye-trackers, which point the camera to the subject and record their pupils' position. The imagery is interpreted by software, and the eyes' positions are extracted, as well as any blinks and the pupillary response, how much the pupils dilate and trick. This information is recorded in the form of a time series of the x and y position of where each subject's eyes are looking.

From this data, we can extract several features. The position of one's gaze on the page could itself be a valuable point of information, usually referred to as areas of interest.

Pupil dilation in and of itself has been shown to have direct relationships with how one processes data presented one is presented with. As such pupillary response over time is a promising feature. Blinking can, in the same way, give us some indication of how one is processing information.

A fixation in attracting is when your gaze rests on a particular point for a certain amount of time fixation would usually indicate a higher level of attention to that specific region of the screen.

Saccades are the rapid eye movement between two fixations. Information is not processed during a saccade. However, we can still learn something about how one processes information and the information being processed. For example, one would see a higher degree of saccades for texts that consist of longer and more complicated words.

The duration of the saccades and fixations, the lengths of saccades, and the relationship between saccades and fixations in the dataset can give us insight into how the subject processes information.

The features we are engineering in this thesis are primarily higher-order features built on top of the lower order features that we have just mentioned.



LHIPA citenp:[duchowskiLowHighIndex2020]


=== Generalizability


== Datasets

---

Michalis Comments on the Datasets:
I think that we should have a Datasets section and an Analysis section.

The dataset needs to report the datasets selected (and why we selected those datasets). The description below is light (but I guess you are going to extend), but it doesn't say why those datasets have been selected. See here in this paper: https://dl.acm.org/doi/pdf/10.1145/3363384.3363392 How the dataset has been reported in section 3. See also the pre-processing pipeline (as a seperate follow up section). So a good suggestion is presenting the datasets in this section and in the next section presenting a by-dataset pre-processing. Then you can have the analysis.

- see also here: https://link.springer.com/chapter/10.1007/978-3-030-58558-7_22 the data charachteristics.

Currently this section is very heavy and mixes everything (e.g., datasets description, preprocessing and the analysis), you will need to devide and describe each parts.

For the data analtsis you will need to describe how the datasets were analyzed to respond to the research questions/objectives of the thesis. In other words, what is the process you employed to achieve the goal. You needs to describe the systematic process employed to analyze the numeric information collected. The analysis section needs to cover the formal statistical analysis approach employed. For instance is can say “To identify generalizable features, we conduct an exhaustive search of possible analyses and data combinations....". In fact see section 4 of the IMWUT paper and try to describe your pipeline, at the same level of granularity (i.e., allowing the reader to replicate).

Also specify how the analyses were conducted, e.g., the analyses were performed with SPSS 25.0 for Windows.

---


We have been working with three different datasets gathered and published by other researchers.


=== EMIP

The Eye-Movements In Programming (EMIP) dataset is a large eye-tracking dataset collected as a community effort involving 11 research teams across four continents.
The goal was to provide a substantially large dataset open and free to stimulate research relating to development and eye-tracking.
216 programmers of differing experience levels were recorded while performing two code comprehension tasks.
In addition to the eye-tracking information, a wealth of metadata is also provided. citenp:[bednarikEMIPEyeMovements2020]

The recording was performed using a screen-mounted SMI RED25 mobile video-based eye tracker.
Stimuli were presented on a laptop computer screen with a resolution of 1920 x 1080 pixels. citenp:[bednarikEMIPEyeMovements2020]

The participants were primarily university students enrolled in computing courses but included academic and administrative staff and professional programmers.
There were 41 female participants and 175 male participants.
The mean age was 26.56 years with a standard deviation of 9.28. citenp:[bednarikEMIPEyeMovements2020]


=== CSCW

CSCW is a product of a study run by Sharma et al. citenp:[sharmaLookingLookingDual2015].
In this study, 98 university students partook in an exercise designed to show the different gaze patterns of learnings using a Massive Open Online Course (MOOC).

The study used a pre-test as a contextual primer.
The primer came in two different forms, one text-based test, and one schema-based test.
Participants that got the textual primer were denoted T, and participants that got the schematic primer were denoted S.

After the primer, participants watched a video from Khan Academy on the topic of "resting membrane potential."
Arranged in 16 TT pairs, 16 SS pairs, and 17 ST pairs, each pair collaborated to create a concept map using IHMC CMap tools.

The participants were recorded with SMI RED 250 eye-trackers as they completed the individual video task and the collaborative concept mapping task.
After completing the pre-test, the video task, and the concept map, the participants also completed a post-test.

While the concept mapping task was cooperative, all measurements are individual.
We will be working with the data on the individual level.
The data is also split into one part for the video watching phase and one part for the concept mapping phase.
We will not be considering any links between the two and will treat them as separate.


=== Fractions

The dataset that we refer to as fractions was gathered by Olson et al. citenp:[olsenUsingIntelligentTutoring2014].
It is an eye-tracking dataset from an experiment intending to investigate the differences between individual and collaborative performance when working on conceptually or procedurally oriented problems.
The study included 84 4th and 5th grades from two US elementary schools.
The students completed either individual tasks or collaborative tasks using an interactive tutoring system developed by the researchers.
Participants in the study also completed a pretest on the morning of the experiment, and a post-test the day after.
The results of the pre- and post-test are included with the data.

The students selected for the collaborative tasks were paired by their teachers to ensure that the students collaborate effectively.
They completed tasks in the interactive tutoring system, communicating verbally through a skype connection.
They did not transmit any video signal.

Our dataset consists of only the data used by Sharma et al. citenp:[sharmaMeasuringCausalityCollaborative2021] This only includes the data from the pairs that worked on the collaborative tasks, not the students that worked individually.

== Implementation

Our goal with this system is to create a platform on which we can perform our feature generalizability experiments efficiently and consistently.

In order to achieve this goal, multiple components have to be present.

. We need methods to standardize datasets, so the units are the same and the data is in the same form.
. We need to clean the data to achieve high data quality which can produce good features
. We need a platform that can generate computationally expensive features for multiple large datasets
. We need a platform that can run multiple concurrent pipelines for combinations of datasets, features, and methods for dimensionality reduction
. We need an evaluation step that collects the results from all the pipelines, and can prove pipelines generalizable.
. We need complete reproducibility.

=== Preprocessing

This subsection explains how we achieved goal 1 & 2 of creating a platform for generating generalizable features.

==== Standardization of Datasets
We have found three datasets from different experiments with different contexts.
They also vary in units used and the name of the columns.
Some of the datasets measure time in milliseconds, while others measure it in microseconds.
The datasets also use different names for the same attributes.
These were renamed to a consistent naming scheme.
Some of the subjects were missing labels, we solved this by removing the sample.
We also fixed inconsistencies such as wrong capitalizations of filenames.
The scripts for standardization can be found at Github. In misc/fix*

Something about us generating fixations for EMIP from rawdata

Below we describe in detail how we standardized each of the datasets.

===== EMIP
We changed the dataset to make it easier to handle.

. Created a new column for the status for each timeframe containing "CALIBRATION", "READING", "TEST"
. Created a new column for which trial they were performing
. Removed rows for where the values were all 0, as that could be interpreted as nan.

Preprocessing

. Remove 0 values as they are nan
.

==== Data cleaning
The datasets contains missing values


==== Normalization and Outlier removal
As our subjects comes from multiple contexts, the need for normalization and outlier removals is extra apparent.
The baseline for a subjects pupil dialation is very sensitive to lighting and how well rested you are, so it is important to normalize it.
We chose to min-max normalize the pupil diameter in the range of 0 to 1 per subject.

// The normalized x and y postitions is only used in the entropy feature so it should maybe be mentioned there
The screen sizes in the different experiments where the datasets were from are different. So we normalized the x and y positions in a 1000 by 1000 grid.

As we are working on fixations our sense of time is discretized to the start of each fixation.
But there can be artificially large periods of time between fixations, due to blinking, the subject looking away from the screen or technical malfunction on the equipment.
To mitigate this we remove the outliers by setting a threshold of 1000 ms for saccade duration, and all timegaps over 1000 ms were set to 1000ms,





=== Feature generation

To save computational time, we chose to separate the feature generation and the model training in to two separate jobs. This subsection explains how we achieved goal 3.

==== Flow
The flow of the feature generation job is as follows:

. Download the datasets from google cloud storage
. Load data as a list of pandas dataframes
. Standardize data. This step is different for each dataset
. Normalize data
. Generate aggregated attributes, such as saccade duration, saccade_length and angle of saccades.
. Take the rolling mean of the signals
. Generate features
. Upload features to google cloud storage

==== Features
The features we generate can be separated into 3 different groups based on how they were made.

* Timeseries Features
* Eyetracking Features
* Heatmap features

==== Timeseries Features
Agnostic features, they are a description of the signal, not the meaning behind the signal.
The signals we used are pupil diameter, fixation duration, saccade duration and saccade length.

Pupil diameter is the average diameter of the pupil over a fixation.
Fixation duration is the duration of a fixation, and is the difference between the endtime and starttime of a fixation.
Saccade duration is the time between two fixations.
Saccade length is the euclidiean distance between the coordinates of two fixations.

From these signals we calculate 5 features.


===== Power Spectral Histogram.
The power spectrum of a time series, decomposes the time series to the frequncies present in the signal, and the amplitude of each of these frequencies.
Once compouted, they can be represented as a histogram which is called the power spectral histogram.
 We computed the centroid, variance, skew and kurtosis of the power spectral histogram.

===== Autoregressive Moving Average model (Arma)
An ARMA process describes a time series with two polynomials.
The first of these polynomials describes the autoregressive part of the timeseries.
The second part describes the moving average.
Arma is formally described by the following formula.

asciimath:[X_t = sum_(j=1)^p phi_j X_(t-j) + sum_(i=1)^q theta_i epsilon_(t-i) + epsilon_t]

The features we extract from arma are extracted with the following algorithm

```
best_fit = null
for p up to 4
    for q up to 4
        fit = arma(timeseries, p, q)
        if(best_fit.aic > fit.aic)
            best_fit = fit
return best_fit["ar"], best_fit["ma"], best_fit["exog"]


```
===== Garch
Garch is similar to Arma, but it is applied to the variance of the data instead of the mean.

We extract features from garch similar to how we extract features from Arma.

```
best_fit = null
for p up to 4
    for q up to 4
        fit = garch(timeseries, p, q)
        if(best_fit.aic > fit.aic)
            best_fit = fit
return best_fit["mu"], best_fit["omega"], best_fit["alpha"], best_fit["gamma"], best_fit["beta"]
```


===== Markov Models
I don't know the reason behind testing markov models, so we need to work that out.

```
normalized_timeseries = (timeseries - min(timeseries)) / (max(timeseries) - min(timeseries))
discretized_timeseries = discretize timeseries in 100 bins
best_fit = null
for i up to 8
   fit = GaussianHMM(covariance_type="full").fit(discretized_timeseries, n_components=i)
   if(best_fit.aic > fit.aic)
           best_fit = fit
flat_transistion_matrix = best_fit.transition_matrix.flatten()
padded_transition_matrix = pad flat_transistion matrix with n zeroes so the length is 64
return padded_transition_matrix
```

===== The Low/High Index of Pupillary Activity (LHIPA)
LHIPA citenp:[duchowskiLowHighIndex2020] is an enhancement to the Index of Pupilary Activity, which is a metric to discriminate cognitive load from pupil diameter oscillation.

For simplicity we extracted the LHIPA metric from all the signals even though it is only proven to have value on pupil diameter signals.


==== Eyetracking features
These are features that are connected to the domain of eye-tracking and not signal processing features.


===== Ratio of Information Processing Ratio


Global information processing (GIP) is often synonymous with skimming text.
Your gaze is jumping around to larger sections of the screen, and not staying at a place for a longer time.
Which results in shorter fixations and longer saccades.

Local information processing (LIP) is the exactly opposite, your gaze is focusing on smaller areas for a longer duration and not moving around that much.

For this metric fixations are measured in time, while saccades are measured in distance.
This is because we are interested in how big the area you are moving around is, and for how long you are focusing on specific areas.

To compute the ratio, we divide GIP by LIP.

The following algorithm extracts the feature:

```
upper_threshold_saccade_length = get 75 percentile of saccade_lengths
lower_threshold_saccade_length = get 25 percentile of saccade_lengths
upper_threshold_fixation_duration = get 75 percentile of fixation_durations
lower_threshold_fixation_duration = get 25 percentile of fixation_durations

LIP = 0
GIP = 0
for saccade_length, fixation_duration in saccade_lengths, fixation_durations
    fixation_is_short = fixation_duration <= lower_threshold_fixation_duration
    fixation_is_long = upper_threshold_fixation_duration <= fixation_duration
    saccade_is_short = saccade_length <= lower_threshold_saccade_length
    saccade_is_long = upper_threshold_saccade_length <= saccade_length

    if fixation_is_long and saccade_is_short:
        LIP += 1
    elif fixation_is_short and saccade_is_long:
        GIP += 1

return GIP / (LIP + 1)
```





===== Skewness of saccade speed
Saccade velocity skewness has been shown to correlate with familiarity.
If the skewness is highly positive, that means that the overall speeds were high.
It means that you knew where to look.

Familiarity and expertise is different. You can know where to look, but have to think twice before doing it.

To calculate this feature we calculated the speed by dividing the saccade length on the saccade duration.
Then we got the skew of the distribution outputted.

===== Verticality of Saccades
Something about reasoning behind looking at horizontality of saccades.

To get the horizontality of saccades we get the angle between every fixation, with respect to the x axis. We do that with arctan2, which outputs the angle in radians between pi and -pi. Since we're only interested in the horizontality of the saccade, we take the absolute value of the angle.
```
radians = atan2(y2 - y1, x2 - x1)
return abs(radians)
```
To describe the horizontality of each point in a range between 0 and 1, we take take sinus of every angle.
```
for angle in angles
   angle = sin(angle)
```
Then we average all the sinus values.

```
verticality = angles.average()
```

===== Entropy of Gaze

We use the entropy of the gaze to compute the focus size of the subject. To calculate it we create a grid of 50 by 50 px bins. And placing each fixation in one of these bins based on which bin its x and y position corresponds to. When we have this grid we flatten it and take the entropy of the resulting distribution.

This tells us if the gaze was evenly spread over the whole screen, or if the subject was more focused on specific areas of the screen. For this feature we used the normalized values for x and y, to keep the number of bins consistent between datasets.

The following algorithm extracts the feature:
```
x_normalized = normalize x between 0 and 1000
y_normalized = normalize y between 0 and 1000

x_axis = [50, 100, 150 ... ,1000]
y_axis = [50, 100, 150 ... ,1000]
2d_histogram = 2d_histogram(xaxis, yaxis, x_normalized, y_normalized)
return entropy(2d_histogram.flatten())

```



==== Heatmaps
As shown in cite:[K paper about heatmaps], heatmaps of the gaze can predict performance in learning activities.

The heatmaps for emip we generated ourselves with a python library called heatmappy.

. Split each subjects into 30 partitions
. Created a 1920 * 1080 image
. Plotted the x,y postions with heatmappy
. Resized the image to 175*90

This will return a list of heatmaps per subject.

From the heatmaps used a pretrained vgg19 model with the imagenet weights to generate a feature vector of size 1000 features per image

1. Scale the images down using the preprocess_input function found in `keras.applications.image_netutils`
2. Use the pretrained VGG-19 model to extract features per image
3. Flatten the matrix to a single list of values

===== Pseudocode
```
frames = Split each subject into 30 partitions
features = []
for frame in frames
    image = image of with dimensions 1920, 1080
    heatmap = heatmappy.heatmap_on_img(frame.x_and_y_postions, image)
    scaled_down_heatmap = keras.applications.image_netutils(heatmap)
    heatmap_features = vgg19model.predict(scaled_down_heatmap)
    features.append(heatmap_feature.flatten())
return features.flatten()
```

==== Final Feature set
After feature generation these are the features that has been generated per subject.

include::feature_set_table.adoc[]

=== Classification

This section explains how we solved goal 4 and 5 of creating our platform.
We need a platform that can run multiple concurrent pipelines for combinations of datasets, features, and methods for dimensionality reduction

==== Flow

* Download features from Google Cloud Storage
*


==== Grouping of features

Since it would be too computationally exhaustive to test all combinations of the features, we separated them into 13 different feature groups, which we will test.

The features are divided in this way:

===== Feature Groups

include::feature_groups_table.adoc[]

=== Dimensionality Reduction / Feature Selection
All of our pipelines does either feature selection or Dimensionality reduction.
The method we use for dimensionality reduction is PCA and the one for Feature selection is Lasso.
For all pipelines we also use a zero-variance filter to remove the features that has no variance in its data

==== Dimensionality Reduction
Dimensionality reduction is the process of mapping the current feature space to another feature space with a lower dimensionality.
We use PCA for dimensionality reduction.


==== Feature Selection
Feature selection is the process of reducing the number of features used by removing features that give less information.
We use Lasso as our feature selection alorithm because it performs well when the number of samples is less than the number of features.
Which is often the case in our pipelines.
For instance the heatmap feature group contains 153600 features per subject.

=== Prediction: Ensemble Learning
Our classifier is a weighted voting ensemble with a KNN-regressor, Random forest regressor and a Support Vector regressor.
To find the weights we do a cross-validation and set the respective weights as 1 - Root Mean square error.

=== Training, validation and testing setup
To evaluate our pipelines we do two types of testing, *out-of-sampling-testing* and *out-of-study-testing*.

To perform out of sampling testing, we split our in study dataset in three parts.
Training, validation, and testing data.
The training data is used to train the models, the validation set is used weight the different classifiers in the voting ensemble.
The test data is to evaluate the model on unseen data.

Out of study testing is how we can evaluate the generalizibility of the pipeline.
We are using two out of three datasets to train and evaluate the model with out of sampling testing.
The out of study testing is done with the dataset we do not use in the in study testing.
If EMIP and fractions are used for in study, then cscw is used for out of study testing.
Since all the datasets are from different contexts, we can evaluate how our pipelines generalize over contexts.

==== Evaluation Criteira

All of our pipelines are evaluated with Root mean squared error.
Since the labels are normalized in a range from 0 to 1 before training, our RMSE is equivalent to NRMSE, so we can reliably compare the results from one dataset to another.
RMSE is calculated by the following formula.


:asciimath:[RMSE = sqrt ((sum_(i=1)^text(Number of samples) (text(predicted)_i - text(original)_i)^2) / text(Number of Samples))]

=== Feature Generalizability Index
I think we can wait with this section until we have decided on a metric. I want to incorporate random-baseline


=== Bench-marking the Generalizable Features




[%header,format=csv]
|===
// include::results_filtered.csv[]
|===

==== Reproducability
This section explains how we reached goal 6 of our platform.
* We need complete reproducibility.

Our reproducibility strategy primarily consists of two different components.
The version-control tool, git; and the machine learning management tool comet.ml.

==== Git
Git keeps track of all versions of our source-code.
Our system is set up to demand that all local changes to the code be committed to git before a run in the cloud will be allowed.
We ensure that all our parameters are represented in the code.
This in turn ensures that we always know the state of the code responsible for each experiment.
When we run an experiment in the cloud we log the start parameters of the system and the hash associated with the commit.

==== comet.ml
comet.ml is a machine learning management tool. It can handle user-management, visualization, tracking of experiments, and much more.
In our case we use it to track the results of our experiements, and how they relate to eachother.

==== Seeding
All of our experiments ran with seeded randomness. Our implementation for seeding

```
seed = 69420
np.random.seed(seed)
random.seed(seed)
os.environ["PYTHONHASHSEED"] = str(seed)
tf.random.set_seed(seed)
```

== Results and Discussion

---

Michalis Comments on the Results and Discussion:

I am not sure if it is a good idea to have the results together with the discussion section. I know that in the IMWUT paper are together, and you can have them together, but it is not as clean.

The results section speaks for itself: report on the results of your work in an organized way. This section is typically staying away from discussing the importance of the results or describing any potential implications and focusing on just reporting the results. Try to use charts, graphs, and tables as appropriate (use IMWUT paper, or any of the others I sent to you as examples).

For the Discussion section: In this section you will need to interpret the results. 

First summarises the main findings and relate them back to the initial problem of the thesis set out to do (i.e., the RQs). The summary of the findings and connection to the RQs should come early in the discussion section. 

Then, start interpreting them (e.g., What do the results mean? What are the reasons behind results that you obtained? What did the results tell you in light of the related works?). Each of the findings need to be discussed in detail and interpret the findings against related published works (e.g., confirming, falsify and/or extending them). This step is essential to show how contribution (how your work adds to, complements or clarifies the current body of knowledge). The goal of this section, is to enable the reader to understand what the findings mean. Connections are usually made back to the knowledge established in the introduction and the related work sections, showing how the new study’s findings responded to the RQs and closed the knowledge gap described in the introduction.

---



== Conclusion and Further Work

---

Michalis Comments on the Conclusion and Further Work:


This is a very short section the following points:

- summarizes the contributions of this work and clarifies that you have delivered what has been promised in the RQs/Introduction.
- highlights any key points which you would like the reader to remember (i.e., take-home message).
- stresses the specific significance of this work and presents a call for future research that would likely extend the reported study’s findings (e.g., in case this work opens avenues for new research).

Since the contributions have already been reported and discussed in the previous sections, it is important to "zoom out" in this section and try to see the wood rather than the tree. Copying and pasting the same text that describes the contribution in other sections should be avoided, however, a limited repetition (rewritten in a simplified manner) can be okay. Try to frame the contributions of the work in a way that a generalist can understand their value, not just for researchers in this particular narrow topic. 

The last part of this section is used to describe the avenues of future research that have been opened from your results and establish a call for future research (e..g., that would extend/complement/exploit your findings). Sometimes researchers use this part to describe their future research. It is often better to offer a few well-thought and important future steps than a "mainstream" list of smaller items (e.g., collecting additional data, implementing a similar study).

Again, use IMWUT paper as an example or any other of the papers I have provided. You can also see the theses I have shared with you.


---


bibliography::[]
